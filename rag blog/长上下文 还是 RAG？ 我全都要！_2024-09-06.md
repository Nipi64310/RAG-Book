嘿，大家好！这里是一个专注于AI智能体的频道！

最近，深度学习和人工智能领域的大牛们在arXiv上发表了一篇有趣的研究，标题挺长的：《检索增强生成或长上下文大型语言模型？全面研究和混合方法》。

今天分享一篇由Google DeepMind和密歇根大学的研究人员发布的一个长上下文和RAG混合的方法。

> Retrieval Augmented Generation or Long-Context LLMs?
 AComprehensive Study and Hybrid Approach

> 检索增强生成（RAG）一直是大型语言模型（LLMs）高效处理超长文本的强大工具。然而，像Gemini-1.5和GPT-4这样的最新LLM展现了直接理解长文本的卓越能力。我们对RAG和长文本（LC）LLM进行了全面的比较，旨在利用两者的优点。我们使用三种最新的LLM，在各种公开数据集上对RAG和LC进行基准测试。结果显示，**当资源充足时，LC在平均性能上始终优于RAG。然而，RAG显著较低的成本仍然是一个明显的优势**。基于这一观察，我们提出了Self-Route，这是一种简单但有效的方法，根据模型的自我反思将查询路由到RAG或LC。**Self-Route显著降低了计算成本，同时保持了与LC相当的性能**。我们的研究为使用RAG和LC的长文本应用提供了指导。


首先，让我们聊聊RAG。这种方法让LLM通过检索相关信息来生成回答，就像是给模型加了个外挂，让它能够访问海量信息，而且成本很低。但是，随着LLM的发展，像Gemini1.5和GPT-4这样的模型已经能够直接理解超长文本了。这就引出了一个问题：**我们是否还需要RAG？**

研究人员决定做个全面的比较。他们用最新的三个LLM在不同的公共数据集上进行了基准测试。结果发现，只要资源足够，LC在几乎所有情况下都比RAG表现得更好。但是，RAG的成本优势依然明显。这就是说，虽然LC在理解长文本上更胜一筹，但RAG在成本上更具吸引力。
![](https://files.mdnice.com/user/50285/a5ec0dc2-8ba3-4a52-b460-209b868ed546.png)


基于这个发现，研究人员提出了一种新方法，叫做SELF-ROUTE。这种方法根据模型的自我评估来决定是使用RAG还是LC。SELF-ROUTE在保持与LC相当的性能的同时，显著降低了计算成本。例如，在Gemini-1.5-Pro上，成本降低了65%，在GPT-4上降低了39%。

研究人员还深入分析了RAG与LC的预测差异，发现RAG和LC在很多情况下会给出相同的预测，无论是正确的还是错误的。事实上，对于 63% 的查询，模型预测是完全相同的；对于 70% 的查询，分数差异小于 10（绝对值）。有趣的是，相同的预测不一定正确，如代表平均分数的不同颜色所示，即(S_RAG + S_LC) / 2。这一观察结果表明，RAG 和 LC 不仅倾向于做出相同的正确预测，而且倾向于做出类似的错误。

![](https://files.mdnice.com/user/50285/6a966f1f-f402-4b7d-a2c0-f6467dd595ee.png)

因此，我们可以在大多数查询中利用 RAG，为一小部分真正擅长的查询保留计算成本更高的 LC。通过这样做，RAG 可以在不牺牲整体性能的情况下显着降低计算成本。

SELF-ROUTE也比较简单，其实就俩步骤：先是RAG加Route这一步，然后是长上下文预测那一步。前一步里，我们把查询和检索到的内容块儿给LLM，然后让它预测这查询能不能回答，如果能，就生成答案。这跟咱们平时用的RAG差不多，但有个关键的不同点：**LLM现在有个选择权，如果觉得根据提供的内容回答不了问题，它可以选择不回答，提示语就是“Write unanswerable if the query can not be answered based on the provided text”。**

对于那些LLM觉得能回答的查询，我们就直接接受RAG的预测作为最终答案。对于那些LLM觉得回答不了的，咱们就进入第二步，把完整的上下文信息给长上下文LLM，让它来得出最终预测，也就是LC。评测发现，虽然RAG得分稳定低于LC，但是SELF-ROUTE可以用更少的tokens，获得接近甚至更好的效果

![](https://files.mdnice.com/user/50285/67ffa87e-e90b-45e8-8f99-9a6a09b42bcf.png)


好了，这就是我今天想分享的内容。如果你对构建AI智能体感兴趣，别忘了点赞、关注噢~