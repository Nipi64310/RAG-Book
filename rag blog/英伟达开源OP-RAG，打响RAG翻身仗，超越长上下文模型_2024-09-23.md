嘿，大家好！这里是一个专注于AI智能体的频道！

前面写过一次抱抱脸的Agent框架了，这次提供一个完整实验脚本，以及看看它的设计逻辑。再一次给大家安利一下，确实比较简单实用。

今天分享一个英伟达的最新研究，属于是在长上下文阶段为RAG的一次辩护。因为随着长文本LLM的出现，这些模型能处理更长的文本序列，RAG似乎变得不那么重要了。

> In Defense of RAG in the Era of Long-Context Language Models

> 检索增强生成（RAG）克服了早期LLMs中有限的上下文限制，过去一直是基于上下文的答案生成的可靠解决方案。最近，长上下文LLMs的出现使得模型能够包含更长的文本序列，这使得 RAG 的吸引力下降。**最近的研究表明，长上下文LLMs在长上下文应用中显着优于 RAG**。与现有的偏爱长语境LLM而不是 RAG 的研究不同，**我们认为LLMs中的极长语境会导致对相关信息的关注度降低，并导致答案质量的潜在下降**。本文重新审视长上下文答案生成中的 RAG。**我们提出了一种顺序保留检索增强生成（OP-RAG）机制，该机制显着提高了 RAG 在长上下文问答应用中的性能。使用OP-RAG，随着检索块数量的增加，答案质量先上升，然后下降，形成倒U形曲线**。与将整个上下文作为输入的长上下文LLM相比，OP-RAG 可以用更少的标记获得更高的答案质量。对公共基准的大量实验证明了我们的 OP-RAG 的优越性。

OP-RAG是个什么东西呢？

传统的RAG，检索与查询最相关的前k个文本块。按照相似度排序之后，按照一定的顺序放在大模型的prompt中。

与传统RAG将检索到的文本块按相关性降序排列不同，OP-RAG保持了这些文本块在原文中的顺序。也就是说，如果一个文本块在原文中出现在另一个文本块之前，那么在处理答案时，它也会被放在前面。

![](https://files.mdnice.com/user/50285/c1e7346f-5363-4983-bcdc-6d2910fb5d9f.png)

论文里做了一系列的实验，结果表明，OP-RAG在长文本问答任务中的表现比传统的RAG和长文本LLM都要好。特别是在使用Llama3.1-70B模型时，OP-RAG在只使用16K tokens的情况下，就达到了44.43的F1分数，而没有使用RAG的Llama3.1-70B，即使用了128K tokens，也只得到了34.32的F1分数。

![](https://files.mdnice.com/user/50285/05cd77ed-03c1-4f75-8052-94a818e0f805.png)

![](https://files.mdnice.com/user/50285/f1f50c65-c9a5-434c-97ba-5daa029a44a7.png)


好了，这就是我今天想分享的内容。如果你对构建AI智能体感兴趣，别忘了点赞、关注噢~